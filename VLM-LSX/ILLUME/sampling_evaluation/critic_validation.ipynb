{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "308cdf9d",
   "metadata": {},
   "source": [
    "# Critic Feedback and Model Evaluation\n",
    "This notebook provides the necessary scirpts to filter generated explanations using an automated critic based on ground truth reference. \n",
    "\n",
    "Additionally, it includes code to calculate scores for evaluation on validation and test splits\n",
    "\n",
    "The subsequent code assume the following file structure for ILLUME training. The epoch directories of each iteration contain the csvs generated on the validation split for different training epochs, whereas the training directory contains multiple subdirectories for csvs generated at different temperatures on the train split.\n",
    "If one evaluation (i.e. von leaf directory) is parallelized and split into multiple csv files, it is expected that all files are sorted alphatically by their respective split of the data. This is automatically taken care of when using the ```--split```argument of ```vqax_magma_eval.py```\n",
    "\n",
    "```\n",
    "workspace\n",
    "│       \n",
    "└─── repositories \n",
    "│   │\n",
    "│   └─── ILLUME\n",
    "│       │\n",
    "│       └─── results\n",
    "│           │\n",
    "│           └─── vqax\n",
    "│               │\n",
    "│               └─── it0\n",
    "│               │   │\n",
    "│               │   └─── train\n",
    "│               │       │                  \n",
    "│               │       └─── 0.01\n",
    "│               │       └─── 0.1\n",
    "│               │       └─── ...\n",
    "│               │\n",
    "│               └─── it1 \n",
    "│               │   │ \n",
    "│               │   └─── epoch1\n",
    "│               │   └─── epoch2\n",
    "│               │   └─── epoch...\n",
    "│               │   └─── train\n",
    "│               │       │                  \n",
    "│               │       └─── 0.01\n",
    "│               │       └─── 0.1\n",
    "│               │       └─── ...\n",
    "│               │\n",
    "│               └─── it...\n",
    "└─── datasets \n",
    "    │\n",
    "    └─── COCO\n",
    "        │\n",
    "        └─── VQA-X\n",
    "            └─── train.json\n",
    "            └─── val.json\n",
    "            └─── test.json\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126d6a1",
   "metadata": {},
   "source": [
    "## Feedback on generated samples\n",
    "Generate any number of explanations beforehand using ```vqax_magma_eval.py``` at various temperatures. \n",
    "If the file structure differs from the one discussed above this might require adjustments of the code below.\n",
    "\n",
    "### 1) Load generated explanations into training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce62cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Set this parameter to the iteration in question (use 0 for the first iteration)\n",
    "iteration = 0\n",
    "#####################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from nlgeval import NLGEval\n",
    "import re\n",
    "\n",
    "def isfloat(str_):\n",
    "      return re.match(r'^-?\\d+(?:\\.\\d+)$', str_) is not None\n",
    "\n",
    "\n",
    "\n",
    "nlgeval = NLGEval(no_glove=True, no_skipthoughts=True)\n",
    "\n",
    "expl_gen_cnt = 0\n",
    "\n",
    "\n",
    "df_train = pd.read_json('/workspace/datasets/COCO/VQA-X/train.json')\n",
    "df_train.reset_index(inplace=True)\n",
    "\n",
    "root = '../results/vqax/'\n",
    "it_root = os.path.join(root, f'it{iteration}', 'train')\n",
    "\n",
    "mappings = {}\n",
    "for root, dirs, files in os.walk(it_root):\n",
    "\n",
    "    temps = [x for x in dirs if x.isnumeric() or isfloat(x)]\n",
    "    break\n",
    "\n",
    "\n",
    "for temp in temps:\n",
    "    print(temp)\n",
    "    temp_path =  os.path.join(it_root, temp)\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(temp_path):\n",
    "        file_paths = [os.path.join(temp_path, file) for file in sorted(files)]\n",
    "        break \n",
    "    \n",
    "    \n",
    "\n",
    "    if type(file_paths) == str or len(file_paths) == 1:\n",
    "        df_tmp = pd.read_csv(file_paths[0])\n",
    "        df_tmp.fillna('', inplace=True)\n",
    "    else:\n",
    "        dfs = []\n",
    "        for path in file_paths:\n",
    "            dfs.append(pd.read_csv(path))\n",
    "        df_tmp = pd.concat(dfs)\n",
    "        \n",
    "        df_tmp.reset_index(inplace=True)\n",
    "        assert(len(df_tmp) == len(df_train))\n",
    "        df_tmp.fillna('', inplace=True)\n",
    "        \n",
    "    \n",
    "    for expl_column in df_tmp.loc[:, df_tmp.columns.str.startswith('gen_explanation')].columns:\n",
    "        df_train[f'gen_explanation{expl_gen_cnt}'] = df_tmp[expl_column].apply(lambda x: str(x).lower().strip())\n",
    "        mappings[expl_gen_cnt] = float(temp)\n",
    "        expl_gen_cnt += 1\n",
    "    df_train['gen_answer'] = df_tmp['gen_answer']\n",
    "    df_train.gen_answer.fillna('', inplace=True)\n",
    "    df_train['overlap'] = df_train.apply(lambda x: max(x['multiple_choice_answer'].count(x['gen_answer']), x['gen_answer'].count(x['multiple_choice_answer']))>0, axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a52b1f",
   "metadata": {},
   "source": [
    "### 2) Calculate per sample Rouge-L score\n",
    "Depending on the number of generated samples this may run for a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0f610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29459/29459 [02:31<00:00, 194.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "df_train['explanation_reference'] = df_train.explanation.apply(lambda x: x.strip().lower())\n",
    "\n",
    "\n",
    "scores = list()\n",
    "\n",
    "for index, e in tqdm(df_train.iterrows(), total=len(df_tmp)):\n",
    "    refs = [e.explanation_reference]\n",
    "    refs_clean = []\n",
    "    for ref in refs:\n",
    "        if len(ref) > 0:\n",
    "            refs_clean.append(ref)\n",
    "    scores_sample = list()\n",
    "    for hyp_column in df_train.loc[:, df_train.columns.str.startswith('gen_explanation')].columns:\n",
    "        hyp = e[hyp_column]\n",
    "        if len(hyp) < 3:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            score = nlgeval.compute_individual_metrics(refs_clean, hyp)['ROUGE_L']\n",
    "        scores_sample.append(score)\n",
    "    scores.append(scores_sample)\n",
    "\n",
    "df_train.insert(12, 'scores', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77489b4",
   "metadata": {},
   "source": [
    "### 3) Load the training data of the previous iteration\n",
    "Include already filtered samples from previous iterations. There won't be any for iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36bef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous training samples detected\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_prev = pd.read_json(os.path.join(it_root, '..', 'train_samples.json'))\n",
    "except: \n",
    "    df_prev = None\n",
    "    print('No previous training samples detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c0fc0",
   "metadata": {},
   "source": [
    "### 4) Apply critic\n",
    "\n",
    "Filter for explanations with at R-L score over threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3ee804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29459/29459 [00:04<00:00, 7226.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at least one fitting explanation for 1184 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Set critic thhreshold\n",
    "threshold = 0.7\n",
    "#######################\n",
    "\n",
    "from tqdm import tqdm\n",
    "it_samples = []\n",
    "cnt = 0\n",
    "for row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "    idx, item = row\n",
    "    ref_row = df_train.iloc[idx]\n",
    "    image_id = item['image_id']\n",
    "    question = item['question']\n",
    "    question_id = item['question_id']\n",
    "    answer = ref_row['multiple_choice_answer']\n",
    "    scores = np.array(item['scores'])\n",
    "    inds = np.array(np.where(scores >= threshold)) \n",
    "    exps = []\n",
    "    if(len(inds[0])>0):\n",
    "        cnt+=1\n",
    "    for ind in inds[0]:\n",
    "        exps.append(item[f'gen_explanation{ind}'])\n",
    "        \n",
    "    if df_prev:\n",
    "        exp_prev = df_prev.loc[df_prev.st].loc[df_prev.question_id == question_id]\n",
    "        for row_prev in exp_prev.iterrows():\n",
    "            idx_prev, item_prev = row_prev\n",
    "            exps.append(item_prev['explanation_sample'])\n",
    "        \n",
    "    for exp in set(exps):\n",
    "        it_samples.append({'image_id': image_id, 'question_id':question_id, 'question': question, 'answer': answer, 'explanation_sample': exp})\n",
    "        \n",
    "    \n",
    "\n",
    "print(f'Found at least one fitting explanation for {cnt} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171433f2",
   "metadata": {},
   "source": [
    "### 5) Prepare training data for next iteration\n",
    "Draw additional VQA only samples for more robust training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f02d0578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of self-generated samples: 1367\n"
     ]
    }
   ],
   "source": [
    "df_it = pd.DataFrame(it_samples)\n",
    "df_it['st'] = True\n",
    "print(f'Number of self-generated samples: {len(df_it)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8be33e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10000 addtional VQA samples\n",
      "Total number of training samples: 11367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4495/2920992128.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_st = df_it.append(df_sample)\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Set number of additional VQA-Only Samples (We recommend roughly 10x the number of explanations samples)\n",
    "vqa_samples = 10000\n",
    "seed = 42\n",
    "#######################\n",
    "\n",
    "df_train['pos_sample'] = df_train.question_id.apply(lambda x: len(df_it.loc[df_it.question_id == x]) == 0)\n",
    "df_sample = df_train.loc[df_train.pos_sample].sample(n=min(vqa_samples, len(df_train.loc[df_train.pos_sample])), random_state=seed)\n",
    "print(f'Added {len(df_sample)} addtional VQA samples')\n",
    "df_sample['st'] = False\n",
    "df_st = df_it.append(df_sample)\n",
    "df_st.reset_index(inplace=True, drop=True)\n",
    "df_st['sample_id'] = df_st.index\n",
    "\n",
    "print(f'Total number of training samples: {len(df_st)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012744e",
   "metadata": {},
   "source": [
    "### 6) Store training data for next iteration\n",
    "This assumes the file structure mentioned above. Change if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c821900",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_root = f'../results/vqax/it{iteration+1}'\n",
    "os.makedirs(next_root, exist_ok=True)\n",
    "df_st.to_json(os.path.join(next_root, 'train_samples.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5c1b4",
   "metadata": {},
   "source": [
    "## Evaluation on validation split\n",
    "\n",
    "As stated above assumes file structure that contain ```epoch...``` directories in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cce1d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mEpoch | B-1        | B-2         | B-3        | B-4         | METEOR     | ROUGE_L    | CIDEr       | Q/A Acc\u001B[0m\n",
      "epoch1| 42.54      | 30.44       | 20.73      | 14.06       | 14.96      | 39.52      | 44.57       | 80.66 |\n",
      "\u001B[1mEpoch | B-1        | B-2         | B-3        | B-4         | METEOR     | ROUGE_L    | CIDEr       | Q/A Acc\u001B[0m\n",
      "epoch2| 43.06      | 30.73       | 21.16      | 14.53       | 14.82      | 39.60      | 46.05       | 82.85 |\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Set this parameter to the iteration in question \n",
    "iteration = 1\n",
    "#####################\n",
    "\n",
    "import pandas as pd\n",
    "from nlgeval import NLGEval\n",
    "import os\n",
    "nlgeval = NLGEval(no_glove=True, no_skipthoughts=True)\n",
    "\n",
    "\n",
    "root = '../results/vqax/'\n",
    "it_root = os.path.join(root, f'it{iteration}', )\n",
    "\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(it_root):\n",
    "    epochs = [x for x in  dirs if 'epoch' in x]\n",
    "    break\n",
    "\n",
    "for epoch in sorted(epochs):\n",
    "    epoch_path = os.path.join(it_root, epoch)\n",
    "\n",
    "    file_paths = None\n",
    "    for root, dirs, files in os.walk(epoch_path):\n",
    "        file_paths = [os.path.join(epoch_path, file) for file in sorted(files)]\n",
    "        break \n",
    "    \n",
    "    df_val = pd.read_json('/workspace/datasets/COCO/VQA-X/val.json')\n",
    "    df_val.reset_index(inplace=True)\n",
    "    \n",
    "    df_tmp = None\n",
    "    if type(file_paths) == str or len(file_paths) == 1:\n",
    "        df_tmp = pd.read_csv(file_paths[0])\n",
    "        df_tmp.fillna('', inplace=True)\n",
    "    else:\n",
    "        dfs = []\n",
    "\n",
    "        for path in file_paths:\n",
    "            dfs.append(pd.read_csv(path))\n",
    "        if len(dfs) == 0:\n",
    "            print(f'Skipping epoch {epoch}')\n",
    "            continue\n",
    "        df_tmp = pd.concat(dfs)\n",
    "        \n",
    "        \n",
    "        #df_tmp.drop_duplicates(subset=['image_id'], inplace=True)\n",
    "        df_tmp.reset_index(inplace=True)\n",
    "        assert(len(df_tmp) == len(df_val))\n",
    "        df_tmp.fillna('', inplace=True)\n",
    "    \n",
    "    \n",
    "    df_val['gen_answer'] = df_tmp['gen_answer'].astype(str)\n",
    "    df_val['overlap'] = df_val.apply(lambda x: (max(x['multiple_choice_answer'].count(str(x['gen_answer'])), str(x['gen_answer']).count(x['multiple_choice_answer']))>0) and len(x['gen_answer'].strip()) >0  , axis = 1)\n",
    "    expl_gen_cnt = 0\n",
    "    for expl_column in df_tmp.loc[:, df_tmp.columns.str.startswith('gen_explanation')].columns:\n",
    "        df_val[f'gen_explanation{expl_gen_cnt}'] = df_tmp[expl_column].apply(lambda x: str(x).lower().strip())\n",
    "        expl_gen_cnt += 1\n",
    "\n",
    "    df_val['explanation_reference'] = df_val.explanation.apply(lambda x: x.strip().lower())\n",
    "    df_val['explanation_reference_2'] = df_val.explanation_2.apply(lambda x: x.strip().lower())\n",
    "    df_val['explanation_reference_3'] = df_val.explanation_3.apply(lambda x: x.strip().lower())\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    hypothesis = list(df_val.gen_explanation1.values)\n",
    "    references = [list(df_val.explanation_reference.values), \n",
    "                  list(df_val.explanation_reference_2.values), \n",
    "                  list(df_val.explanation_reference_3.values)]\n",
    "    metrics_dict = nlgeval.compute_metrics(ref_list=references, hyp_list=hypothesis)\n",
    "    metrics_dict\n",
    "    print(f\"\\033[1mEpoch | B-1        | B-2         | B-3        | B-4         | METEOR     | ROUGE_L    | CIDEr       | Q/A Acc\\033[0m\")\n",
    "    print(f\"{epoch}| {metrics_dict['Bleu_1']*100:0.2f}      | {metrics_dict['Bleu_2']*100:0.2f}       | {metrics_dict['Bleu_3']*100:0.2f}      | {metrics_dict['Bleu_4']*100:0.2f}       | {metrics_dict['METEOR']*100:0.2f}      | {metrics_dict['ROUGE_L']*100:0.2f}      | {metrics_dict['CIDEr']*100:0.2f}       | {len(df_val.loc[df_val.overlap])/len(df_val)*100:0.2f} |\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff291d9f",
   "metadata": {},
   "source": [
    "## Evaluation on test split\n",
    "\n",
    "This assumes the explanation for a single (best) epoch to be in the given directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d527c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m B-1       | B-2         | B-3        | B-4         | METEOR     | ROUGE_L    | CIDEr       | Q/A Acc\u001B[0m\n",
      "50.32      | 37.20       | 26.62      | 19.01       | 16.52      | 44.24      | 60.18       | 85.48 |\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Directory of explanations generated on test split. Can contain one or multiple csv files\n",
    "test_eval_path = '../results/test/ILLUME/'\n",
    "#####################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "df_test = pd.read_json('/workspace/datasets/COCO/VQA-X/test.json')\n",
    "\n",
    "# import os\n",
    "import pandas as pd\n",
    "from nlgeval import NLGEval\n",
    "nlgeval = NLGEval(no_glove=True, no_skipthoughts=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_paths = None\n",
    "for root, dirs, files in os.walk(test_eval_path):\n",
    "    file_paths = [os.path.join(test_eval_path, file) for file in sorted(files)]\n",
    "    break \n",
    "\n",
    "\n",
    "df_tmp = None\n",
    "if type(file_paths) == str or len(file_paths) == 1:\n",
    "    df_tmp = pd.read_csv(file_paths[0])\n",
    "    df_tmp.fillna('', inplace=True)\n",
    "else:\n",
    "    dfs = []\n",
    "\n",
    "    for path in file_paths:\n",
    "        dfs.append(pd.read_csv(path))\n",
    "    if len(dfs) == 0:\n",
    "        print(f'Skipping epoch {epoch}')\n",
    "    df_tmp = pd.concat(dfs)\n",
    "\n",
    "    df_tmp.reset_index(inplace=True)\n",
    "    assert(len(df_tmp) == len(df_test))\n",
    "    df_tmp.fillna('', inplace=True)\n",
    "\n",
    "\n",
    "df_test['gen_answer'] = df_tmp['gen_answer'].astype(str)\n",
    "df_test['overlap'] = df_test.apply(lambda x: (max(x['multiple_choice_answer'].count(str(x['gen_answer'])), str(x['gen_answer']).count(x['multiple_choice_answer']))>0) and len(x['gen_answer'].strip()) >0  , axis = 1)\n",
    "expl_gen_cnt = 0\n",
    "for expl_column in df_tmp.loc[:, df_tmp.columns.str.startswith('gen_explanation')].columns:\n",
    "    df_test[f'gen_explanation{expl_gen_cnt}'] = df_tmp[expl_column].apply(lambda x: str(x).lower().strip())\n",
    "    expl_gen_cnt += 1\n",
    "\n",
    "df_test['explanation_reference'] = df_test.explanation.apply(lambda x: x.strip().lower())\n",
    "df_test['explanation_reference_2'] = df_test.explanation_2.apply(lambda x: x.strip().lower())\n",
    "df_test['explanation_reference_3'] = df_test.explanation_3.apply(lambda x: x.strip().lower())\n",
    "\n",
    "\n",
    "hypothesis = list(df_test.gen_explanation1.values)\n",
    "references = [list(df_test.explanation_reference.values), \n",
    "              list(df_test.explanation_reference_2.values), \n",
    "              list(df_test.explanation_reference_3.values)]\n",
    "metrics_dict = nlgeval.compute_metrics(ref_list=references, hyp_list=hypothesis)\n",
    "metrics_dict\n",
    "print(f\"\\033[1m B-1       | B-2         | B-3        | B-4         | METEOR     | ROUGE_L    | CIDEr       | Q/A Acc\\033[0m\")\n",
    "print(f\"{metrics_dict['Bleu_1']*100:0.2f}      | {metrics_dict['Bleu_2']*100:0.2f}       | {metrics_dict['Bleu_3']*100:0.2f}      | {metrics_dict['Bleu_4']*100:0.2f}       | {metrics_dict['METEOR']*100:0.2f}      | {metrics_dict['ROUGE_L']*100:0.2f}      | {metrics_dict['CIDEr']*100:0.2f}       | {len(df_test.loc[df_test.overlap])/len(df_test)*100:0.2f} |\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}